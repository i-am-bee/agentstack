# BeeAI Platform: ACP to A2A Migration Guide

The BeeAI platform is transitioning from ACP (Agent Communication Protocol) to A2A (Agent2Agent) Protocol starting with version 0.3.x. This guide walks you through migrating your existing agents with practical examples and clear explanations.

The BeeAI team has designed this migration to be as seamless as possible. Most of your existing agent logic will work with minimal changes.

## What's Changing?

### SDK: `acp_sdk` → `beeai_sdk`

The primary change is replacing the `acp_sdk` dependency with `beeai_sdk`. The new SDK provides:
- Full A2A compatibility while maintaining familiar patterns
- Higher-level constructs that simplify agent development
- Complete integration with the BeeAI Platform
- Enhanced extension system for improved modularity

### Key Architectural Changes

1. **Metadata System → Extensions System:** Rich metadata capabilities through specialized extensions
2. **Function Signatures:** Updated to use A2A types and context patterns
3. **Message Handling:** Simplified message processing with new Message types
4. **Platform Integration:** Enhanced platform services through extension injection

## Migration Checklist

✅ Replace `acp_sdk` with `beeai_sdk` in dependencies<br>
✅ Update import statements<br>
✅ Replace metadata with A2A agent detail<br>
✅ Update agent decorator parameters<br>
✅ Update function signatures<br>
✅ Update extension injection patterns
✅ Update error handling patterns

## Step-by-Step Migration

### 1. Update Dependencies

Update your `pyproject.toml` or `requirements.txt`:

**Old (ACP)**

```toml
dependencies = [
    "acp_sdk>=1.0.0",
    # other dependencies
]
```

**New (A2A)**

```toml
dependencies = [
    "beeai_sdk>=0.3.0",
    # other dependencies
]
```

### 2. Update Import Statements

**Old (ACP)**

```
from acp_sdk import Annotations, MessagePart, Metadata
from acp_sdk.models import Message
from acp_sdk.models.models import CitationMetadata, TrajectoryMetadata
from acp_sdk.models.platform import AgentToolInfo, PlatformUIAnnotation, PlatformUIType
from acp_sdk.server import Context, RunYield, RunYieldResume, Server
```

**New (A2A)**

```
from a2a.types import AgentCapabilities, AgentSkill, Message
from beeai_sdk.server import Server
from beeai_sdk.server.context import RunContext
from beeai_sdk.a2a.extensions import (
    AgentDetail, 
    AgentDetailTool, 
    CitationExtensionServer, 
    CitationExtensionSpec, 
    TrajectoryExtensionServer, 
    TrajectoryExtensionSpec,
    LLMServiceExtensionServer,
    LLMServiceExtensionSpec
)
from beeai_sdk.util.file import load_file
```

### 3. Update Agent Decorator

**Old (ACP)**

<details>
  <summary>Expand ACP Metadata Example</summary>

```
from acp_sdk import Annotations, Metadata

@server.agent(
    name="jennas_granite_chat",
    description="This is a general-purpose chat assistant prototype built with the BeeAI Framework and powered by Granite. It leverages the experimental `RequirementAgent` with `ConditionalRequirement` rules to intelligently decide when to use tools—specifically `ThinkTool` for reasoning and `DuckDuckGoSearchTool` for fetching real-time information.\n\nThe implementation uses specific conditional requirements: `ThinkTool` is forced at step 1 and after any other tool execution (with `consecutive_allowed=False`), while `DuckDuckGoSearchTool` is limited to 2 invocations maximum and includes custom checks that skip search for casual messages like \"hi\" or \"thanks.\"\n\nIt maintains conversation context using `UnconstrainedMemory` with session-based storage and implements comprehensive trajectory metadata logging throughout all interaction steps. Search results are automatically processed through regex-based citation extraction that converts markdown links `[text](URL)` into structured `CitationMetadata` objects for the platform's citation GUI support.\n\nThe agent includes error handling with try-catch blocks that provide clear, helpful messages when issues occur, and uses the `is_casual()` function to intelligently determine when tools aren't necessary for simple conversational exchanges.",
    metadata=Metadata(
        annotations=Annotations(
            beeai_ui=PlatformUIAnnotation(
                ui_type=PlatformUIType.CHAT,
                user_greeting="Hi! I'm your Granite-powered AI assistant—here to help with questions, research, and more. What can I do for you today?",
                display_name="Jenna's Granite Chat",
                tools=[
                    AgentToolInfo(name="Think", description="Advanced reasoning and analysis to provide thoughtful, well-structured responses to complex questions and topics."),
                    AgentToolInfo(name="DuckDuckGo", description="Search the web for current information, news, and real-time updates on any topic.")
                ]
            )
        ),
        author={"name": "Jenna Winkler"},
        contributors=[{"name": "Tomas Weiss"}, {"name": "Tomas Dvorak"}],
        recommended_models=["granite3.3:8b-beeai"],
        tags=["Granite", "Chat", "Research"], framework="BeeAI", license="Apache 2.0",
        links=[{"type": "source-code", "url": "https://github.com/jenna-winkler/granite_chat"}])
```

</details>

**New (A2A)**

<details>
  <summary>Expand A2A Agent Detail Example</summary>

```
from beeai_sdk.a2a.extensions import AgentDetail, AgentDetailTool

@server.agent(
    name="Jenna's Granite Chat",
    default_input_modes=["text", "text/plain", "application/pdf", "text/csv", "application/json"],
    default_output_modes=["text", "text/plain"],
    detail=AgentDetail(
        interaction_mode="multi-turn",
        user_greeting="Hi! I'm your Granite-powered AI assistant. How can I help?",
        version="0.0.10",
        tools=[
            AgentDetailTool(
                name="Think", 
                description="Advanced reasoning and analysis to provide thoughtful, well-structured responses to complex questions and topics."
            ),
            AgentDetailTool(
                name="DuckDuckGo", 
                description="Search the web for current information, news, and real-time updates on any topic."
            ),
            AgentDetailTool(
                name="File Processing", 
                description="Read and analyze uploaded files including PDFs, text files, CSV data, and JSON documents."
            )
        ],
        framework="BeeAI",
        author={
            "name": "Jenna Winkler"
        },
        source_code_url="https://github.com/jenna-winkler/granite_chat"
    ),
    capabilities=AgentCapabilities(streaming=True),
    skills=[
        AgentSkill(
            id="chat",
            name="Chat",
            description=dedent(
                """\
                The agent is an AI-powered conversational system designed to process user messages, maintain context,
                generate intelligent responses, and analyze uploaded files.
                """
            ),
            tags=["Chat", "Files"],
            examples=[
                "What are the latest advancements in AI research from 2025?",
                "What's the difference between LLM tool use and API orchestration?",
                "Can you help me draft an email apologizing for missing a meeting?",
                "Analyze this CSV file and tell me the key trends.",
                "Summarize the main points from this PDF document.",
            ]

        )
    ],
)
```

4. Update Function Signatures
5. Update Message Processing
6. Update Trajectory Logging
7. Update Citation Handling
8. Update Response Yielding
.......TBC!

## Full Code Example

<details>
  <summary>Expand to see A2A Chat Agent Code Example</summary>

```py
import logging
import os
from typing import Annotated
from collections import defaultdict
from textwrap import dedent

from a2a.types import (
    AgentSkill,
    Message,
)
from beeai_framework.adapters.openai import OpenAIChatModel
from beeai_framework.agents.experimental import (
    RequirementAgent,
)
from beeai_framework.agents.experimental.events import (
    RequirementAgentSuccessEvent,
)
from beeai_framework.agents.experimental.utils._tool import FinalAnswerTool
from beeai_framework.backend.types import ChatModelParameters
from beeai_framework.emitter import EventMeta
from beeai_framework.memory import UnconstrainedMemory
from beeai_framework.middleware.trajectory import GlobalTrajectoryMiddleware
from beeai_framework.tools import Tool
from beeai_framework.tools.search.duckduckgo import DuckDuckGoSearchTool
from beeai_framework.tools.search.wikipedia import WikipediaTool

from beeai_framework.tools.weather import OpenMeteoTool

from beeai_sdk.a2a.extensions import (
    AgentDetail,
    AgentDetailTool,
    CitationExtensionServer,
    CitationExtensionSpec,
    TrajectoryExtensionServer,
    TrajectoryExtensionSpec,
    LLMServiceExtensionServer,
    LLMServiceExtensionSpec,
)
from beeai_sdk.a2a.extensions.services.platform import PlatformApiExtensionServer, PlatformApiExtensionSpec
from beeai_sdk.a2a.types import AgentMessage, AgentArtifact
from beeai_sdk.server import Server
from beeai_sdk.server.context import RunContext
from chat.tools.general.current_time import CurrentTimeTool
from chat.helpers.citations import extract_citations
from chat.helpers.trajectory import TrajectoryContent
from openinference.instrumentation.beeai import BeeAIInstrumentor

from chat.tools.files.file_creator import FileCreatorTool, FileCreatorToolOutput
from chat.tools.files.file_reader import create_file_reader_tool_class
from chat.tools.files.utils import FrameworkMessage, extract_files, to_framework_message
from chat.tools.general.act import (
    ActAlwaysFirstRequirement,
    ActTool,
    act_tool_middleware,
)
from chat.tools.general.clarification import (
    ClarificationTool,
    clarification_tool_middleware,
)

# Temporary instrument fix
EventMeta.model_fields["context"].exclude = True

BeeAIInstrumentor().instrument()
logging.getLogger("opentelemetry.exporter.otlp.proto.http._log_exporter").setLevel(logging.CRITICAL)
logging.getLogger("opentelemetry.exporter.otlp.proto.http.metric_exporter").setLevel(logging.CRITICAL)

logger = logging.getLogger(__name__)

messages: defaultdict[str, list[Message]] = defaultdict(list)
framework_messages: defaultdict[str, list[FrameworkMessage]] = defaultdict(list)

server = Server()


@server.agent(
    name="Chat",
    documentation_url=(
        f"https://github.com/i-am-bee/beeai-platform/blob/{os.getenv('RELEASE_VERSION', 'main')}"
        "/agents/official/beeai-framework/chat"
    ),
    version="1.0.0",
    default_input_modes=["text", "text/plain"],
    default_output_modes=["text", "text/plain"],
    detail=AgentDetail(
        interaction_mode="multi-turn",
        user_greeting="How can I help you?",
        tools=[
            AgentDetailTool(
                name="Act Tool",
                description="Auxiliary tool that ensures thoughtful tool selection by requiring explicit reasoning and tool choice before executing any other tool.",
            ),
            AgentDetailTool(
                name="Clarification Tool",
                description="Enables the agent to ask clarifying questions when user requirements are unclear, preventing assumptions and ensuring accurate task completion.",
            ),
            AgentDetailTool(
                name="Wikipedia Search",
                description="Fetches summaries and information from Wikipedia articles.",
            ),
            AgentDetailTool(
                name="Weather Information (OpenMeteo)",
                description="Provides real-time weather updates and forecasts.",
            ),
            AgentDetailTool(
                name="Web Search (DuckDuckGo)",
                description="Retrieves real-time search results from the web.",
            ),
            AgentDetailTool(
                name="File Reader",
                description="Reads and returns content from uploaded or generated files.",
            ),
            AgentDetailTool(
                name="File Creator",
                description="Creates new files with specified content and metadata, uploading them to the platform for download or further processing.",
            ),
            AgentDetailTool(
                name="Current Time",
                description="Provides current date and time information.",
            ),
        ],
        framework="BeeAI",
    ),
    skills=[
        AgentSkill(
            id="chat",
            name="Chat",
            description=dedent(
                """\
                The agent is an AI-powered conversational system designed to process user messages, maintain context,
                and generate intelligent responses. Built on the **BeeAI framework**, it leverages memory and external
                tools to enhance interactions. It supports real-time web search, Wikipedia lookups, file manipulations,
                and weather updates, making it a versatile assistant for various applications.

                ## How It Works
                The agent processes incoming messages and maintains a conversation history using an **unconstrained
                memory module**. It utilizes a language model (`CHAT_MODEL`) to generate responses and can optionally
                integrate external tools for additional functionality. The agent is basically a ReAct agent built on 
                top of a Requirement agent. It uses auxiliary tools like Act Tool and Clarification Tool to enhance its
                capabilities for smaller models.

                It supports:
                - **Web Search (DuckDuckGo)** – Retrieves real-time search results.
                - **Wikipedia Search** – Fetches summaries from Wikipedia.
                - **Weather Information (OpenMeteo)** – Provides real-time weather updates.
                - **File Reader** – Reads and returns content from uploaded files.
                - **File Creator** – Creates new files with specified content and metadata.
                - **Current Time** – Provides current date and time information.

                The agent also includes an **event-based streaming mechanism**, allowing it to send partial responses
                to clients as they are generated.
            
                ## Key Features
                - **Conversational AI** – Handles multi-turn conversations with memory.
                - **Tool Integration** – Supports real-time search, Wikipedia lookups, files manipulations, and weather updates.
                - **Event-Based Streaming** – Can send partial updates to clients as responses are generated.
                """
            ),
            tags=["chat"],
            examples=["Please find a room in LA, CA, April 15, 2025, checkout date is april 18, 2 adults"],
        )
    ],
)
async def chat(
    message: Message,
    context: RunContext,
    trajectory: Annotated[TrajectoryExtensionServer, TrajectoryExtensionSpec()],
    citation: Annotated[CitationExtensionServer, CitationExtensionSpec()],
    llm_ext: Annotated[LLMServiceExtensionServer, LLMServiceExtensionSpec.single_demand()],
    _: Annotated[PlatformApiExtensionServer, PlatformApiExtensionSpec()],
):
    """
    The agent is an AI-powered conversational system with memory, supporting real-time search, Wikipedia lookups,
    and weather updates through integrated tools.
    """
    extracted_files = await extract_files(history=messages[context.context_id], incoming_message=message)
    input = to_framework_message(message)

    # Configure tools
    file_reader_tool_class = create_file_reader_tool_class(
        extracted_files
    )  # Dynamically created tool input schema based on real provided files ensures that small LLMs can't hallucinate the input

    FinalAnswerTool.description = dedent("""\
        Assemble and send the final answer to the user. When using information gathered from other tools that provided URL addresses, you MUST properly cite them using markdown citation format: [description](URL).

        # Citation Requirements:
        - Use descriptive text that summarizes the source content
        - Include the exact URL provided by the tool
        - Place citations inline where the information is referenced

        # Examples:
        - According to [OpenAI's latest announcement](https://example.com/gpt5), GPT-5 will be released next year.
        - Recent studies show [AI adoption has increased by 67%](https://example.com/ai-study) in enterprise environments.
        - Weather data indicates [temperatures will reach 25°C tomorrow](https://weather.example.com/forecast).
        """)  # type: ignore

    tools = [
        # Auxiliary tools
        ActTool(),  # Enforces correct thinking sequence by requiring tool selection before execution
        ClarificationTool(),  # Allows agent to ask clarifying questions when user requirements are unclear
        # Common tools
        WikipediaTool(),
        OpenMeteoTool(),
        DuckDuckGoSearchTool(),
        file_reader_tool_class(),
        FileCreatorTool(),
        CurrentTimeTool(),
    ]

    requirements = [
        ActAlwaysFirstRequirement(),  #  Enforces the ActTool to be used before any other tool execution.
    ]

    llm_conf = None
    if llm_ext and llm_ext.data:
        [llm_conf] = llm_ext.data.llm_fulfillments.values()

    llm = OpenAIChatModel(
        model_id=llm_conf.api_model if llm_conf else os.getenv("LLM_MODEL", "llama3.1"),
        api_key=llm_conf.api_key if llm_conf else os.getenv("LLM_API_KEY", "dummy"),
        base_url=llm_conf.api_base if llm_conf else os.getenv("LLM_API_BASE", "http://localhost:11434/v1"),
        parameters=ChatModelParameters(temperature=0.0),
        tool_choice_support=set(),
    )

    # Build dynamic instructions based on available files
    base_instructions = dedent(
        """\
        You are a helpful AI assistant built on the BeeAI framework. You have access to various tools and capabilities to assist users effectively.

        ## Core Behavior Guidelines:
        - Always be helpful, accurate, and concise in your responses
        - Use the Act Tool before executing any other tools to ensure thoughtful reasoning
        - When user requirements are unclear, use the Clarification Tool to ask specific questions
        - Maintain conversation context and refer to previous messages when relevant

        ## Tool Usage:
        - **Act Tool**: Required before using any other tool - explain your reasoning and tool choice
        - **Clarification Tool**: Ask clarifying questions when user intent is ambiguous
        - **Web Search (DuckDuckGo)**: For real-time information, current events, and general web searches
        - **Wikipedia Search**: For encyclopedic information and factual summaries
        - **Weather Tool (OpenMeteo)**: For current weather conditions and forecasts
        - **File Reader**: To read content from uploaded files (when available)
        - **File Creator**: To create new files with specific content and metadata
        - **Current Time**: For date and time information

        ## Citation Requirements:
        When using information from tools that provide URLs, you MUST cite sources using markdown format:
        - Format: [descriptive text](URL)
        - Place citations inline where information is referenced
        - Use descriptive text that summarizes the source content

        ## File Handling:
        - When files are available, reference them by ID and filename
        - Read file contents when users ask about uploaded documents
        - Create files when users need downloadable content
        {file_context}

        ## Response Quality:
        - Provide comprehensive, well-structured answers
        - Break down complex topics into digestible sections
        - Use appropriate formatting (headers, lists, code blocks) when helpful
        - Always complete tasks fully before providing final answers
        """
    )

    if extracted_files:
        files_context = "\n\n## Currently Available Files:"
        files_context += "\nThe user has uploaded the following files that you can access using the File Reader tool:"
        for file in extracted_files:
            files_context += f"\n- **{file.file.filename}** (ID: {file.file.id}) - Available at: {file.file.url}"
        files_context += (
            "\n\nWhen referencing these files, use their ID with the File Reader tool to access their content."
        )
        instructions = base_instructions.format(file_context=files_context)
    else:
        instructions = base_instructions.format(file_context="")

    # Create agent
    agent = RequirementAgent(
        llm=llm,
        tools=tools,
        memory=UnconstrainedMemory(),
        requirements=requirements,
        middlewares=[
            GlobalTrajectoryMiddleware(included=[Tool]),  # ChatModel,
            act_tool_middleware,
            clarification_tool_middleware,
        ],
    )

    messages[context.context_id].append(message)
    framework_messages[context.context_id].append(input)

    await agent.memory.add_many(framework_messages[context.context_id])
    final_answer = None

    async for event, meta in agent.run():
        if not isinstance(event, RequirementAgentSuccessEvent):
            continue

        last_step = event.state.steps[-1] if event.state.steps else None
        if last_step and last_step.tool is not None:
            trajectory_content = TrajectoryContent(
                input=last_step.input,
                output=last_step.output,
                error=last_step.error,
            )
            yield trajectory.trajectory_metadata(
                title=last_step.tool.name,
                content=trajectory_content.model_dump_json(),
            )

            if isinstance(last_step.output, FileCreatorToolOutput):
                result = last_step.output.result
                for file_info in result.files:
                    part = file_info.file.to_file_part()
                    part.file.name = file_info.display_filename
                    yield AgentArtifact(name=file_info.display_filename, parts=[part])

        if event.state.answer is not None:
            # Taking a final answer from the state directly instead of RequirementAgentRunOutput to be able to use the final answer provided by the clarification tool
            final_answer = event.state.answer

    if final_answer:
        framework_messages[context.context_id].append(final_answer)

        citations, clean_text = extract_citations(final_answer.text)

        message = AgentMessage(
            text=clean_text,
            metadata=(citation.citation_metadata(citations=citations) if citations else None),
        )
        messages[context.context_id].append(message)
        yield message


def serve():
    server.run(
        host=os.getenv("HOST", "127.0.0.1"),
        port=int(os.getenv("PORT", 8000)),
        configure_telemetry=True,
    )


if __name__ == "__main__":
    serve()

```

</details>

Source: https://github.com/i-am-bee/beeai-platform/blob/main/agents/official/beeai-framework/chat/src/chat/agent.py

## Key Changes Explained

1. Metadata → A2A extensions

## Available Extensions
