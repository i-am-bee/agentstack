# BeeAI Platform: ACP to A2A Migration Guide

Migrate from ACP (Agent Communication Protocol) to A2A (Agent2Agent) Protocol for BeeAI platform v0.3.x+.

This guide walks you through migrating your BeeAI agents from the legacy ACP SDK to the new BeeAI SDK powered by the A2A protocol. The migration involves updating dependencies, imports, function signatures, and adopting new patterns for metadata, citations, and trajectory logging.

## Migration Checklist

✅ Replace `acp_sdk` with `beeai_sdk` in dependencies<br>
✅ Update imports<br>
✅ Replace Metadata with AgentDetail<br>
✅ Update agent function signature<br>
✅ Refactor message processing logic<br>
✅ Update citation handling
✅ Update trajectory logging
✅ Add file processing capabilities
✅ Verify LLM configuration

## Step-by-Step Migration

### 1. Update Dependencies

**Old (ACP)**

```toml
dependencies = [
    "acp_sdk>=1.0.0",
    # other dependencies
]
```

**New (A2A)**

```toml
dependencies = [
    "beeai_sdk>=0.3.0",
    # other dependencies
]
```

### 2. Update Imports

**Old (ACP)**

```
from acp_sdk import Annotations, MessagePart, Metadata
from acp_sdk.models import Message
from acp_sdk.models.models import CitationMetadata, TrajectoryMetadata
from acp_sdk.models.platform import AgentToolInfo, PlatformUIAnnotation, PlatformUIType
from acp_sdk.server import Context, RunYield, RunYieldResume, Server
```

**New (A2A)**

```
from a2a.types import AgentCapabilities, AgentSkill, Message
from beeai_sdk.server import Server
from beeai_sdk.server.context import RunContext
from beeai_sdk.a2a.extensions import (
    AgentDetail, AgentDetailTool, 
    CitationExtensionServer, CitationExtensionSpec, 
    TrajectoryExtensionServer, TrajectoryExtensionSpec,
    LLMServiceExtensionServer, LLMServiceExtensionSpec
)
from beeai_sdk.util.file import load_file
```

### 3. Update Agent Decorator

**Old (ACP)**

<details>
  <summary>Expand ACP Metadata Example</summary>

```
@server.agent(
    name="jennas_granite_chat",
    description="This is a general-purpose chat assistant prototype built with the BeeAI Framework and powered by Granite.",
    metadata=Metadata(
        annotations=Annotations(
            beeai_ui=PlatformUIAnnotation(
                ui_type=PlatformUIType.CHAT,
                user_greeting="Hi! I'm your Granite-powered AI assistant—here to help with questions, research, and more. What can I do for you today?",
                display_name="Jenna's Granite Chat",
                tools=[
                    AgentToolInfo(name="Think", description="Advanced reasoning and analysis to provide thoughtful, well-structured responses to complex questions and topics."),
                    AgentToolInfo(name="DuckDuckGo", description="Search the web for current information, news, and real-time updates on any topic.")
                ]
            )
        ),
        author={"name": "Jenna Winkler"},
        contributors=[{"name": "Tomas Weiss"}, {"name": "Tomas Dvorak"}],
        recommended_models=["granite3.3:8b-beeai"],
        tags=["Granite", "Chat", "Research"], framework="BeeAI", license="Apache 2.0",
        links=[{"type": "source-code", "url": "https://github.com/jenna-winkler/granite_chat"}])
```

</details>

**New (A2A)**

<details>
  <summary>Expand A2A Agent Detail Example</summary>

```
@server.agent(
    name="Jenna's Granite Chat",
    default_input_modes=["text", "text/plain", "application/pdf", "text/csv", "application/json"],
    default_output_modes=["text", "text/plain"],
    detail=AgentDetail(
        interaction_mode="multi-turn",
        user_greeting="Hi! I'm your Granite-powered AI assistant. How can I help?",
        version="0.0.10",
        tools=[
            AgentDetailTool(
                name="Think", 
                description="Advanced reasoning and analysis to provide thoughtful, well-structured responses to complex questions and topics."
            ),
            AgentDetailTool(
                name="DuckDuckGo", 
                description="Search the web for current information, news, and real-time updates on any topic."
            ),
            AgentDetailTool(
                name="File Processing", 
                description="Read and analyze uploaded files including PDFs, text files, CSV data, and JSON documents."
            )
        ],
        framework="BeeAI",
        author={
            "name": "Jenna Winkler"
        },
        source_code_url="https://github.com/jenna-winkler/granite_chat"
    ),
    skills=[
        AgentSkill(
            id="chat",
            name="Chat",
            description=dedent(
                """\
                The agent is an AI-powered conversational system designed to process user messages, maintain context,
                generate intelligent responses, and analyze uploaded files.
                """
            ),
            tags=["Chat", "Files"],
            examples=[
                "What are the latest advancements in AI research from 2025?",
                "What's the difference between LLM tool use and API orchestration?",
                "Can you help me draft an email apologizing for missing a meeting?",
                "Analyze this CSV file and tell me the key trends.",
                "Summarize the main points from this PDF document.",
            ]

        )
    ],
)
```

### 4. Update Function Signature

**Old (ACP)**

```py
async def general_chat_assistant(input: list[Message], context: Context) -> AsyncGenerator[RunYield, RunYieldResume]:
```

**New (A2A)**

```py
async def general_chat_assistant(
    input: Message, 
    context: RunContext,
    citation: Annotated[CitationExtensionServer, CitationExtensionSpec()],
    trajectory: Annotated[TrajectoryExtensionServer, TrajectoryExtensionSpec()],
    llm: Annotated[
        LLMServiceExtensionServer, 
        LLMServiceExtensionSpec.single_demand(
            suggested=("ibm/granite-3-3-8b-instruct", "llama3.1", "gpt-4o-mini")
        )
    ]
):
```

### 5. Update Message Processing

**Old (ACP)**

```py
user_msg = input[-1].parts[0].content if input else "Hello"
```

**New (A2A)**

```py
user_msg = ""
file_content = ""
uploaded_files = []

for part in input.parts:
    part_root = part.root
    if part_root.kind == "text":
        user_msg = part_root.text
    elif part_root.kind == "file":
        uploaded_files.append(part_root)

if not user_msg:
    user_msg = "Hello"
```

### 6. Update Context Management

**Old (ACP)**

```py
def get_memory(context: Context) -> UnconstrainedMemory:
    session_id = getattr(context, "session_id", "default")
    return memories.setdefault(session_id, UnconstrainedMemory())
```

**New (A2A)**

```py
def get_memory(context: RunContext) -> UnconstrainedMemory:
    context_id = getattr(context, "context_id", getattr(context, "session_id", "default"))
    return memories.setdefault(context_id, UnconstrainedMemory())
```

### 7. File Processing

**New!**

```py
import os

from typing import Annotated

from beeai_sdk.server import Server
from beeai_sdk.a2a.types import Message
from beeai_sdk.a2a.extensions.services.platform import (
    PlatformApiExtensionServer,
    PlatformApiExtensionSpec,
)
from beeai_sdk.platform import File
from beeai_sdk.util.file import load_file


server = Server()


@server.agent(
    default_input_modes=["text/plain"]
)
async def example_agent(
    input: Message,
    _: Annotated[PlatformApiExtensionServer, PlatformApiExtensionSpec()],
):
    """Agent that can accept and modify files"""

    for file_part in input.parts:
        file_part_root = file_part.root

        if file_part_root.kind == "file":
            async with load_file(file_part_root) as loaded_content:
                new_file = await File.create(
                    filename=f"processed_{file_part_root.file.name}",
                    content_type=file_part_root.file.mime_type,
                    content=loaded_content.text.encode(),
                )
                yield new_file.to_file_part()

    yield "File Processing Done"


def run():
    server.run(host=os.getenv("HOST", "127.0.0.1"), port=int(os.getenv("PORT", 8000)))


if __name__ == "__main__":
    run()
```

### 8. LLM Configuration

**New!** - Using LLM Service Extension

```py
import os
from typing import Annotated

from a2a.types import Message
from a2a.utils.message import get_message_text
from beeai_sdk.server import Server
from beeai_sdk.a2a.types import AgentMessage
from beeai_sdk.a2a.extensions import LLMServiceExtensionServer, LLMServiceExtensionSpec

server = Server()

@server.agent()
async def example_agent(
    input: Message,
    llm: Annotated[
        LLMServiceExtensionServer,
        LLMServiceExtensionSpec.single_demand(suggested=("ibm/granite-3-3-8b-instruct",))
    ],
):
    """Agent that uses LLM inference to respond to user input"""

    if llm:
        # Extract the user's message
        user_message = get_message_text(input)
        
        # Get LLM configuration
        # Single demand is resolved to default (unless specified otherwise)
        llm_config = llm.data.llm_fulfillments.get("default")
        
        # Use the LLM configuration with your preferred client
        # The platform provides OpenAI-compatible endpoints
        api_model = llm_config.api_model
        api_key = llm_config.api_key
        api_base = llm_config.api_base

        yield AgentMessage(text=f"LLM access configured for model: {api_model}")

def run():
    server.run(host=os.getenv("HOST", "127.0.0.1"), port=int(os.getenv("PORT", 8000)))

if __name__ == "__main__":
    run()
```

### 9. Update Trajectory Logging

**Old (ACP)**

```py
yield MessagePart(metadata=TrajectoryMetadata(
    kind="trajectory", 
    key=str(uuid.uuid4()),
    message="Processing..."
))
```

**New (A2A)**

```py
yield trajectory.trajectory_metadata(
    title="Processing",
    content="Processing message..."
)
```

### 10. Update Citation Handling

**Old (ACP)**

```py
citations.append(CitationMetadata(
    kind="citation", 
    url=url,
    title=title,
    description=description,
    start_index=start, 
    end_index=end
))

for citation in citations:
    yield MessagePart(metadata=citation)
```

**New (A2A)**

```py
citations.append({
    "url": url,
    "title": title,
    "description": description,
    "start_index": start,
    "end_index": end
})

yield citation.citation_metadata(citations=citations)
```

### 11. Update Response Output

**Old (ACP)**

```py
yield MessagePart(content=response_text)
```

**New (A2A)**

```py
yield response_text
```

## Complete Example

<details>
  <summary>Expand to see A2A Chat Agent Code Example</summary>

```py
import logging
import os
from typing import Annotated
from collections import defaultdict
from textwrap import dedent

from a2a.types import (
    AgentSkill,
    Message,
)
from beeai_framework.adapters.openai import OpenAIChatModel
from beeai_framework.agents.experimental import (
    RequirementAgent,
)
from beeai_framework.agents.experimental.events import (
    RequirementAgentSuccessEvent,
)
from beeai_framework.agents.experimental.utils._tool import FinalAnswerTool
from beeai_framework.backend.types import ChatModelParameters
from beeai_framework.emitter import EventMeta
from beeai_framework.memory import UnconstrainedMemory
from beeai_framework.middleware.trajectory import GlobalTrajectoryMiddleware
from beeai_framework.tools import Tool
from beeai_framework.tools.search.duckduckgo import DuckDuckGoSearchTool
from beeai_framework.tools.search.wikipedia import WikipediaTool

from beeai_framework.tools.weather import OpenMeteoTool

from beeai_sdk.a2a.extensions import (
    AgentDetail,
    AgentDetailTool,
    CitationExtensionServer,
    CitationExtensionSpec,
    TrajectoryExtensionServer,
    TrajectoryExtensionSpec,
    LLMServiceExtensionServer,
    LLMServiceExtensionSpec,
)
from beeai_sdk.a2a.extensions.services.platform import PlatformApiExtensionServer, PlatformApiExtensionSpec
from beeai_sdk.a2a.types import AgentMessage, AgentArtifact
from beeai_sdk.server import Server
from beeai_sdk.server.context import RunContext
from chat.tools.general.current_time import CurrentTimeTool
from chat.helpers.citations import extract_citations
from chat.helpers.trajectory import TrajectoryContent
from openinference.instrumentation.beeai import BeeAIInstrumentor

from chat.tools.files.file_creator import FileCreatorTool, FileCreatorToolOutput
from chat.tools.files.file_reader import create_file_reader_tool_class
from chat.tools.files.utils import FrameworkMessage, extract_files, to_framework_message
from chat.tools.general.act import (
    ActAlwaysFirstRequirement,
    ActTool,
    act_tool_middleware,
)
from chat.tools.general.clarification import (
    ClarificationTool,
    clarification_tool_middleware,
)

EventMeta.model_fields["context"].exclude = True

BeeAIInstrumentor().instrument()
logging.getLogger("opentelemetry.exporter.otlp.proto.http._log_exporter").setLevel(logging.CRITICAL)
logging.getLogger("opentelemetry.exporter.otlp.proto.http.metric_exporter").setLevel(logging.CRITICAL)

logger = logging.getLogger(__name__)

messages: defaultdict[str, list[Message]] = defaultdict(list)
framework_messages: defaultdict[str, list[FrameworkMessage]] = defaultdict(list)

server = Server()


@server.agent(
    name="Chat",
    documentation_url=(
        f"https://github.com/i-am-bee/beeai-platform/blob/{os.getenv('RELEASE_VERSION', 'main')}"
        "/agents/official/beeai-framework/chat"
    ),
    version="1.0.0",
    default_input_modes=["text", "text/plain"],
    default_output_modes=["text", "text/plain"],
    detail=AgentDetail(
        interaction_mode="multi-turn",
        user_greeting="How can I help you?",
        tools=[
            AgentDetailTool(
                name="Act Tool",
                description="Auxiliary tool that ensures thoughtful tool selection by requiring explicit reasoning and tool choice before executing any other tool.",
            ),
            AgentDetailTool(
                name="Clarification Tool",
                description="Enables the agent to ask clarifying questions when user requirements are unclear, preventing assumptions and ensuring accurate task completion.",
            ),
            AgentDetailTool(
                name="Wikipedia Search",
                description="Fetches summaries and information from Wikipedia articles.",
            ),
            AgentDetailTool(
                name="Weather Information (OpenMeteo)",
                description="Provides real-time weather updates and forecasts.",
            ),
            AgentDetailTool(
                name="Web Search (DuckDuckGo)",
                description="Retrieves real-time search results from the web.",
            ),
            AgentDetailTool(
                name="File Reader",
                description="Reads and returns content from uploaded or generated files.",
            ),
            AgentDetailTool(
                name="File Creator",
                description="Creates new files with specified content and metadata, uploading them to the platform for download or further processing.",
            ),
            AgentDetailTool(
                name="Current Time",
                description="Provides current date and time information.",
            ),
        ],
        framework="BeeAI",
    ),
    skills=[
        AgentSkill(
            id="chat",
            name="Chat",
            description=dedent(
                """\
                The agent is an AI-powered conversational system designed to process user messages, maintain context,
                and generate intelligent responses. Built on the **BeeAI framework**, it leverages memory and external
                tools to enhance interactions. It supports real-time web search, Wikipedia lookups, file manipulations,
                and weather updates, making it a versatile assistant for various applications.

                ## How It Works
                The agent processes incoming messages and maintains a conversation history using an **unconstrained
                memory module**. It utilizes a language model (`CHAT_MODEL`) to generate responses and can optionally
                integrate external tools for additional functionality. The agent is basically a ReAct agent built on 
                top of a Requirement agent. It uses auxiliary tools like Act Tool and Clarification Tool to enhance its
                capabilities for smaller models.

                It supports:
                - **Web Search (DuckDuckGo)** – Retrieves real-time search results.
                - **Wikipedia Search** – Fetches summaries from Wikipedia.
                - **Weather Information (OpenMeteo)** – Provides real-time weather updates.
                - **File Reader** – Reads and returns content from uploaded files.
                - **File Creator** – Creates new files with specified content and metadata.
                - **Current Time** – Provides current date and time information.

                The agent also includes an **event-based streaming mechanism**, allowing it to send partial responses
                to clients as they are generated.
            
                ## Key Features
                - **Conversational AI** – Handles multi-turn conversations with memory.
                - **Tool Integration** – Supports real-time search, Wikipedia lookups, files manipulations, and weather updates.
                - **Event-Based Streaming** – Can send partial updates to clients as responses are generated.
                """
            ),
            tags=["chat"],
            examples=["Please find a room in LA, CA, April 15, 2025, checkout date is april 18, 2 adults"],
        )
    ],
)
async def chat(
    message: Message,
    context: RunContext,
    trajectory: Annotated[TrajectoryExtensionServer, TrajectoryExtensionSpec()],
    citation: Annotated[CitationExtensionServer, CitationExtensionSpec()],
    llm_ext: Annotated[LLMServiceExtensionServer, LLMServiceExtensionSpec.single_demand()],
    _: Annotated[PlatformApiExtensionServer, PlatformApiExtensionSpec()],
):
    """
    The agent is an AI-powered conversational system with memory, supporting real-time search, Wikipedia lookups,
    and weather updates through integrated tools.
    """
    extracted_files = await extract_files(history=messages[context.context_id], incoming_message=message)
    input = to_framework_message(message)

    # Configure tools
    file_reader_tool_class = create_file_reader_tool_class(
        extracted_files
    )  # Dynamically created tool input schema based on real provided files ensures that small LLMs can't hallucinate the input

    FinalAnswerTool.description = dedent("""\
        Assemble and send the final answer to the user. When using information gathered from other tools that provided URL addresses, you MUST properly cite them using markdown citation format: [description](URL).

        # Citation Requirements:
        - Use descriptive text that summarizes the source content
        - Include the exact URL provided by the tool
        - Place citations inline where the information is referenced

        # Examples:
        - According to [OpenAI's latest announcement](https://example.com/gpt5), GPT-5 will be released next year.
        - Recent studies show [AI adoption has increased by 67%](https://example.com/ai-study) in enterprise environments.
        - Weather data indicates [temperatures will reach 25°C tomorrow](https://weather.example.com/forecast).
        """)  # type: ignore

    tools = [
        # Auxiliary tools
        ActTool(),  # Enforces correct thinking sequence by requiring tool selection before execution
        ClarificationTool(),  # Allows agent to ask clarifying questions when user requirements are unclear
        # Common tools
        WikipediaTool(),
        OpenMeteoTool(),
        DuckDuckGoSearchTool(),
        file_reader_tool_class(),
        FileCreatorTool(),
        CurrentTimeTool(),
    ]

    requirements = [
        ActAlwaysFirstRequirement(),  #  Enforces the ActTool to be used before any other tool execution.
    ]

    llm_conf = None
    if llm_ext and llm_ext.data:
        [llm_conf] = llm_ext.data.llm_fulfillments.values()

    llm = OpenAIChatModel(
        model_id=llm_conf.api_model if llm_conf else os.getenv("LLM_MODEL", "llama3.1"),
        api_key=llm_conf.api_key if llm_conf else os.getenv("LLM_API_KEY", "dummy"),
        base_url=llm_conf.api_base if llm_conf else os.getenv("LLM_API_BASE", "http://localhost:11434/v1"),
        parameters=ChatModelParameters(temperature=0.0),
        tool_choice_support=set(),
    )

    # Build dynamic instructions based on available files
    base_instructions = dedent(
        """\
        You are a helpful AI assistant built on the BeeAI framework. You have access to various tools and capabilities to assist users effectively.

        ## Core Behavior Guidelines:
        - Always be helpful, accurate, and concise in your responses
        - Use the Act Tool before executing any other tools to ensure thoughtful reasoning
        - When user requirements are unclear, use the Clarification Tool to ask specific questions
        - Maintain conversation context and refer to previous messages when relevant

        ## Tool Usage:
        - **Act Tool**: Required before using any other tool - explain your reasoning and tool choice
        - **Clarification Tool**: Ask clarifying questions when user intent is ambiguous
        - **Web Search (DuckDuckGo)**: For real-time information, current events, and general web searches
        - **Wikipedia Search**: For encyclopedic information and factual summaries
        - **Weather Tool (OpenMeteo)**: For current weather conditions and forecasts
        - **File Reader**: To read content from uploaded files (when available)
        - **File Creator**: To create new files with specific content and metadata
        - **Current Time**: For date and time information

        ## Citation Requirements:
        When using information from tools that provide URLs, you MUST cite sources using markdown format:
        - Format: [descriptive text](URL)
        - Place citations inline where information is referenced
        - Use descriptive text that summarizes the source content

        ## File Handling:
        - When files are available, reference them by ID and filename
        - Read file contents when users ask about uploaded documents
        - Create files when users need downloadable content
        {file_context}

        ## Response Quality:
        - Provide comprehensive, well-structured answers
        - Break down complex topics into digestible sections
        - Use appropriate formatting (headers, lists, code blocks) when helpful
        - Always complete tasks fully before providing final answers
        """
    )

    if extracted_files:
        files_context = "\n\n## Currently Available Files:"
        files_context += "\nThe user has uploaded the following files that you can access using the File Reader tool:"
        for file in extracted_files:
            files_context += f"\n- **{file.file.filename}** (ID: {file.file.id}) - Available at: {file.file.url}"
        files_context += (
            "\n\nWhen referencing these files, use their ID with the File Reader tool to access their content."
        )
        instructions = base_instructions.format(file_context=files_context)
    else:
        instructions = base_instructions.format(file_context="")

    # Create agent
    agent = RequirementAgent(
        llm=llm,
        tools=tools,
        memory=UnconstrainedMemory(),
        requirements=requirements,
        middlewares=[
            GlobalTrajectoryMiddleware(included=[Tool]),  # ChatModel,
            act_tool_middleware,
            clarification_tool_middleware,
        ],
    )

    messages[context.context_id].append(message)
    framework_messages[context.context_id].append(input)

    await agent.memory.add_many(framework_messages[context.context_id])
    final_answer = None

    async for event, meta in agent.run():
        if not isinstance(event, RequirementAgentSuccessEvent):
            continue

        last_step = event.state.steps[-1] if event.state.steps else None
        if last_step and last_step.tool is not None:
            trajectory_content = TrajectoryContent(
                input=last_step.input,
                output=last_step.output,
                error=last_step.error,
            )
            yield trajectory.trajectory_metadata(
                title=last_step.tool.name,
                content=trajectory_content.model_dump_json(),
            )

            if isinstance(last_step.output, FileCreatorToolOutput):
                result = last_step.output.result
                for file_info in result.files:
                    part = file_info.file.to_file_part()
                    part.file.name = file_info.display_filename
                    yield AgentArtifact(name=file_info.display_filename, parts=[part])

        if event.state.answer is not None:
            # Taking a final answer from the state directly instead of RequirementAgentRunOutput to be able to use the final answer provided by the clarification tool
            final_answer = event.state.answer

    if final_answer:
        framework_messages[context.context_id].append(final_answer)

        citations, clean_text = extract_citations(final_answer.text)

        message = AgentMessage(
            text=clean_text,
            metadata=(citation.citation_metadata(citations=citations) if citations else None),
        )
        messages[context.context_id].append(message)
        yield message


def serve():
    server.run(
        host=os.getenv("HOST", "127.0.0.1"),
        port=int(os.getenv("PORT", 8000)),
        configure_telemetry=True,
    )


if __name__ == "__main__":
    serve()

```

</details>

Source: https://github.com/i-am-bee/beeai-platform/blob/main/agents/official/beeai-framework/chat/src/chat/agent.py
