# BeeAI Platform: ACP to A2A Migration Guide

Migrate from ACP (Agent Communication Protocol) to A2A (Agent2Agent) Protocol for BeeAI platform v0.3.x+.

## Quick Migration Checklist

✅ Update dependencies: `acp_sdk` → `beeai_sdk`<br>
✅ Update imports and function signature<br>
✅ Replace `Metadata` with `AgentDetail`<br>
✅ Update message processing<br>
✅ Update trajectory and citation handling

## Step-by-Step Migration

### 1. Update Dependencies & Imports

**Old (ACP)**

```toml
dependencies = ["acp_sdk>=1.0.0"]
```

```
from acp_sdk import Message, Metadata, Link, LinkType, Annotations
from acp_sdk.models import MessagePart
from acp_sdk.server import Context, Server
from acp_sdk.models.platform import PlatformUIAnnotation, PlatformUIType, AgentToolInfo
```

**New (A2A)**

```toml
dependencies = ["beeai_sdk>=0.3.0"]
```

```
from a2a.types import AgentSkill, Message
from beeai_sdk.server import Server
from beeai_sdk.server.context import RunContext
from beeai_sdk.a2a.extensions import (
    AgentDetail, AgentDetailTool, 
    CitationExtensionServer, CitationExtensionSpec, 
    TrajectoryExtensionServer, TrajectoryExtensionSpec,
    LLMServiceExtensionServer, LLMServiceExtensionSpec
)
from beeai_sdk.a2a.types import AgentMessage, AgentArtifact
from beeai_sdk.util.file import load_file
```

### 2. Update Agent Decorator & Function Signature

**Old (ACP)**

<details>
  <summary>Expand ACP Example</summary>

```
@server.agent(
    name="jennas_granite_chat",
    description="This is a general-purpose chat assistant prototype built with the BeeAI Framework and powered by Granite.",
    metadata=Metadata(
        annotations=Annotations(
            beeai_ui=PlatformUIAnnotation(
                ui_type=PlatformUIType.CHAT,
                user_greeting="Hi! I'm your Granite-powered AI assistant—here to help with questions, research, and more. What can I do for you today?",
                display_name="Jenna's Granite Chat",
                tools=[
                    AgentToolInfo(name="Think", description="Advanced reasoning and analysis to provide thoughtful, well-structured responses to complex questions and topics."),
                    AgentToolInfo(name="DuckDuckGo", description="Search the web for current information, news, and real-time updates on any topic.")
                ]
            )
        ),
        author={"name": "Jenna Winkler"},
        contributors=[{"name": "Tomas Weiss"}, {"name": "Tomas Dvorak"}],
        recommended_models=["granite3.3:8b-beeai"],
        tags=["Granite", "Chat", "Research"], framework="BeeAI", license="Apache 2.0",
        links=[{"type": "source-code", "url": "https://github.com/jenna-winkler/granite_chat"}])
async def agent_function(input: list[Message], context: Context) -> AsyncGenerator:
```

</details>

**New (A2A)**

<details>
  <summary>Expand A2A Example</summary>

```
@server.agent(
    name="Jenna's Granite Chat",
    default_input_modes=["text", "text/plain", "application/pdf", "text/csv", "application/json"],
    default_output_modes=["text", "text/plain"],
    detail=AgentDetail(
        interaction_mode="multi-turn",
        user_greeting="Hi! I'm your Granite-powered AI assistant. How can I help?",
        version="0.0.10",
        tools=[
            AgentDetailTool(
                name="Think", 
                description="Advanced reasoning and analysis to provide thoughtful, well-structured responses to complex questions and topics."
            ),
            AgentDetailTool(
                name="DuckDuckGo", 
                description="Search the web for current information, news, and real-time updates on any topic."
            ),
            AgentDetailTool(
                name="File Processing", 
                description="Read and analyze uploaded files including PDFs, text files, CSV data, and JSON documents."
            )
        ],
        framework="BeeAI",
        author={
            "name": "Jenna Winkler"
        },
        source_code_url="https://github.com/jenna-winkler/granite_chat"
    ),
    skills=[
        AgentSkill(
            id="chat",
            name="Chat",
            description=dedent(
                """\
                The agent is an AI-powered conversational system designed to process user messages, maintain context,
                generate intelligent responses, and analyze uploaded files.
                """
            ),
            tags=["Chat", "Files"],
            examples=[
                "What are the latest advancements in AI research from 2025?",
                "What's the difference between LLM tool use and API orchestration?",
                "Can you help me draft an email apologizing for missing a meeting?",
                "Analyze this CSV file and tell me the key trends.",
                "Summarize the main points from this PDF document.",
            ]

        )
    ],
)
async def agent_function(
    message: Message,
    context: RunContext,
    trajectory: Annotated[TrajectoryExtensionServer, TrajectoryExtensionSpec()],
    citation: Annotated[CitationExtensionServer, CitationExtensionSpec()],
    llm_ext: Annotated[LLMServiceExtensionServer, LLMServiceExtensionSpec.single_demand()],
):
```

</details>

### 3. Update Message & File Processing

**Old (ACP)**

```py
user_msg = input[-1].parts[0].content if input else "Hello"
```

**New (A2A)**

```py
# Process message parts - A2A receives single Message with multiple parts
user_text = ""
uploaded_files = []

for part in message.parts:
    if part.root.kind == "text":
        user_text = part.root.text
    elif part.root.kind == "file":
        uploaded_files.append(part.root)

# Simple file processing (if needed)
if uploaded_files:
    from beeai_sdk.util.file import load_file
    
    for file_part in uploaded_files:
        async with load_file(file_part) as loaded_content:
            # Process file content as needed
            content = loaded_content.text
            # Use content in your agent logic...

if not user_text:
    user_text = "Hello"
```

### 4. Update Context & Memory

**Old (ACP)**

```py
def get_memory(context: Context) -> UnconstrainedMemory:
    session_id = getattr(context, "session_id", "default")
    return memories.setdefault(session_id, UnconstrainedMemory())
```

**New (A2A)**

```py
def get_memory(context: RunContext) -> UnconstrainedMemory:
    context_id = getattr(context, "context_id", getattr(context, "session_id", "default"))
    return memories.setdefault(context_id, UnconstrainedMemory())
```

### 5. Update LLM Configuration

**Old (ACP)**

```py
os.environ["OPENAI_API_BASE"] = os.getenv("LLM_API_BASE", "http://localhost:11434/v1")
os.environ["OPENAI_API_KEY"] = os.getenv("LLM_API_KEY", "dummy")
llm = ChatModel.from_name(f"openai:{os.getenv('LLM_MODEL', 'llama3.1')}")
```

**New (A2A)**

```py
llm_conf = None
if llm_ext and llm_ext.data:
    [llm_conf] = llm_ext.data.llm_fulfillments.values()

llm = OpenAIChatModel(
    model_id=llm_conf.api_model if llm_conf else os.getenv("LLM_MODEL", "llama3.1"),
    api_key=llm_conf.api_key if llm_conf else os.getenv("LLM_API_KEY", "dummy"),
    base_url=llm_conf.api_base if llm_conf else os.getenv("LLM_API_BASE", "http://localhost:11434/v1"),
    parameters=ChatModelParameters(temperature=0.0),
    tool_choice_support=set(),
)
```

### 6. Update Trajectory & Citations

**Old (ACP)**

```py
yield MessagePart(metadata=TrajectoryMetadata(
    kind="trajectory", 
    key=str(uuid.uuid4()),
    message="Processing..."
))

citations.append(CitationMetadata(
    kind="citation", 
    url=url,
    title=title,
    description=description,
    start_index=start, 
    end_index=end
))
for citation in citations:
    yield MessagePart(metadata=citation)
```

**New (A2A)**

```py
yield trajectory.trajectory_metadata(
    title="Processing",
    content="Processing message..."
)

citations.append({
    "url": url,
    "title": title,
    "description": description,
    "start_index": start,
    "end_index": end
})
yield citation.citation_metadata(citations=citations)
```

### 7. Response Output

**Old (ACP)**

```py
yield MessagePart(content=response_text)
```

**New (A2A)**

```py
yield AgentMessage(text=response_text)
# or simply:
yield response_text
```

## Complete Example

<details>
  <summary>Expand to see A2A Chat Agent Code Example</summary>

```py
import logging
import os
from typing import Annotated
from collections import defaultdict
from textwrap import dedent

from a2a.types import (
    AgentSkill,
    Message,
)
from beeai_framework.adapters.openai import OpenAIChatModel
from beeai_framework.agents.experimental import (
    RequirementAgent,
)
from beeai_framework.agents.experimental.events import (
    RequirementAgentSuccessEvent,
)
from beeai_framework.agents.experimental.utils._tool import FinalAnswerTool
from beeai_framework.backend.types import ChatModelParameters
from beeai_framework.emitter import EventMeta
from beeai_framework.memory import UnconstrainedMemory
from beeai_framework.middleware.trajectory import GlobalTrajectoryMiddleware
from beeai_framework.tools import Tool
from beeai_framework.tools.search.duckduckgo import DuckDuckGoSearchTool
from beeai_framework.tools.search.wikipedia import WikipediaTool

from beeai_framework.tools.weather import OpenMeteoTool

from beeai_sdk.a2a.extensions import (
    AgentDetail,
    AgentDetailTool,
    CitationExtensionServer,
    CitationExtensionSpec,
    TrajectoryExtensionServer,
    TrajectoryExtensionSpec,
    LLMServiceExtensionServer,
    LLMServiceExtensionSpec,
)
from beeai_sdk.a2a.extensions.services.platform import PlatformApiExtensionServer, PlatformApiExtensionSpec
from beeai_sdk.a2a.types import AgentMessage, AgentArtifact
from beeai_sdk.server import Server
from beeai_sdk.server.context import RunContext
from chat.tools.general.current_time import CurrentTimeTool
from chat.helpers.citations import extract_citations
from chat.helpers.trajectory import TrajectoryContent
from openinference.instrumentation.beeai import BeeAIInstrumentor

from chat.tools.files.file_creator import FileCreatorTool, FileCreatorToolOutput
from chat.tools.files.file_reader import create_file_reader_tool_class
from chat.tools.files.utils import FrameworkMessage, extract_files, to_framework_message
from chat.tools.general.act import (
    ActAlwaysFirstRequirement,
    ActTool,
    act_tool_middleware,
)
from chat.tools.general.clarification import (
    ClarificationTool,
    clarification_tool_middleware,
)

EventMeta.model_fields["context"].exclude = True

BeeAIInstrumentor().instrument()
logging.getLogger("opentelemetry.exporter.otlp.proto.http._log_exporter").setLevel(logging.CRITICAL)
logging.getLogger("opentelemetry.exporter.otlp.proto.http.metric_exporter").setLevel(logging.CRITICAL)

logger = logging.getLogger(__name__)

messages: defaultdict[str, list[Message]] = defaultdict(list)
framework_messages: defaultdict[str, list[FrameworkMessage]] = defaultdict(list)

server = Server()


@server.agent(
    name="Chat",
    documentation_url=(
        f"https://github.com/i-am-bee/beeai-platform/blob/{os.getenv('RELEASE_VERSION', 'main')}"
        "/agents/official/beeai-framework/chat"
    ),
    version="1.0.0",
    default_input_modes=["text", "text/plain"],
    default_output_modes=["text", "text/plain"],
    detail=AgentDetail(
        interaction_mode="multi-turn",
        user_greeting="How can I help you?",
        tools=[
            AgentDetailTool(
                name="Act Tool",
                description="Auxiliary tool that ensures thoughtful tool selection by requiring explicit reasoning and tool choice before executing any other tool.",
            ),
            AgentDetailTool(
                name="Clarification Tool",
                description="Enables the agent to ask clarifying questions when user requirements are unclear, preventing assumptions and ensuring accurate task completion.",
            ),
            AgentDetailTool(
                name="Wikipedia Search",
                description="Fetches summaries and information from Wikipedia articles.",
            ),
            AgentDetailTool(
                name="Weather Information (OpenMeteo)",
                description="Provides real-time weather updates and forecasts.",
            ),
            AgentDetailTool(
                name="Web Search (DuckDuckGo)",
                description="Retrieves real-time search results from the web.",
            ),
            AgentDetailTool(
                name="File Reader",
                description="Reads and returns content from uploaded or generated files.",
            ),
            AgentDetailTool(
                name="File Creator",
                description="Creates new files with specified content and metadata, uploading them to the platform for download or further processing.",
            ),
            AgentDetailTool(
                name="Current Time",
                description="Provides current date and time information.",
            ),
        ],
        framework="BeeAI",
    ),
    skills=[
        AgentSkill(
            id="chat",
            name="Chat",
            description=dedent(
                """\
                The agent is an AI-powered conversational system designed to process user messages, maintain context,
                and generate intelligent responses. Built on the **BeeAI framework**, it leverages memory and external
                tools to enhance interactions. It supports real-time web search, Wikipedia lookups, file manipulations,
                and weather updates, making it a versatile assistant for various applications.

                ## How It Works
                The agent processes incoming messages and maintains a conversation history using an **unconstrained
                memory module**. It utilizes a language model (`CHAT_MODEL`) to generate responses and can optionally
                integrate external tools for additional functionality. The agent is basically a ReAct agent built on 
                top of a Requirement agent. It uses auxiliary tools like Act Tool and Clarification Tool to enhance its
                capabilities for smaller models.

                It supports:
                - **Web Search (DuckDuckGo)** – Retrieves real-time search results.
                - **Wikipedia Search** – Fetches summaries from Wikipedia.
                - **Weather Information (OpenMeteo)** – Provides real-time weather updates.
                - **File Reader** – Reads and returns content from uploaded files.
                - **File Creator** – Creates new files with specified content and metadata.
                - **Current Time** – Provides current date and time information.

                The agent also includes an **event-based streaming mechanism**, allowing it to send partial responses
                to clients as they are generated.
            
                ## Key Features
                - **Conversational AI** – Handles multi-turn conversations with memory.
                - **Tool Integration** – Supports real-time search, Wikipedia lookups, files manipulations, and weather updates.
                - **Event-Based Streaming** – Can send partial updates to clients as responses are generated.
                """
            ),
            tags=["chat"],
            examples=["Please find a room in LA, CA, April 15, 2025, checkout date is april 18, 2 adults"],
        )
    ],
)
async def chat(
    message: Message,
    context: RunContext,
    trajectory: Annotated[TrajectoryExtensionServer, TrajectoryExtensionSpec()],
    citation: Annotated[CitationExtensionServer, CitationExtensionSpec()],
    llm_ext: Annotated[LLMServiceExtensionServer, LLMServiceExtensionSpec.single_demand()],
    _: Annotated[PlatformApiExtensionServer, PlatformApiExtensionSpec()],
):
    """
    The agent is an AI-powered conversational system with memory, supporting real-time search, Wikipedia lookups,
    and weather updates through integrated tools.
    """
    extracted_files = await extract_files(history=messages[context.context_id], incoming_message=message)
    input = to_framework_message(message)

    # Configure tools
    file_reader_tool_class = create_file_reader_tool_class(
        extracted_files
    )  # Dynamically created tool input schema based on real provided files ensures that small LLMs can't hallucinate the input

    FinalAnswerTool.description = dedent("""\
        Assemble and send the final answer to the user. When using information gathered from other tools that provided URL addresses, you MUST properly cite them using markdown citation format: [description](URL).

        # Citation Requirements:
        - Use descriptive text that summarizes the source content
        - Include the exact URL provided by the tool
        - Place citations inline where the information is referenced

        # Examples:
        - According to [OpenAI's latest announcement](https://example.com/gpt5), GPT-5 will be released next year.
        - Recent studies show [AI adoption has increased by 67%](https://example.com/ai-study) in enterprise environments.
        - Weather data indicates [temperatures will reach 25°C tomorrow](https://weather.example.com/forecast).
        """)  # type: ignore

    tools = [
        # Auxiliary tools
        ActTool(),  # Enforces correct thinking sequence by requiring tool selection before execution
        ClarificationTool(),  # Allows agent to ask clarifying questions when user requirements are unclear
        # Common tools
        WikipediaTool(),
        OpenMeteoTool(),
        DuckDuckGoSearchTool(),
        file_reader_tool_class(),
        FileCreatorTool(),
        CurrentTimeTool(),
    ]

    requirements = [
        ActAlwaysFirstRequirement(),  #  Enforces the ActTool to be used before any other tool execution.
    ]

    llm_conf = None
    if llm_ext and llm_ext.data:
        [llm_conf] = llm_ext.data.llm_fulfillments.values()

    llm = OpenAIChatModel(
        model_id=llm_conf.api_model if llm_conf else os.getenv("LLM_MODEL", "llama3.1"),
        api_key=llm_conf.api_key if llm_conf else os.getenv("LLM_API_KEY", "dummy"),
        base_url=llm_conf.api_base if llm_conf else os.getenv("LLM_API_BASE", "http://localhost:11434/v1"),
        parameters=ChatModelParameters(temperature=0.0),
        tool_choice_support=set(),
    )

    # Build dynamic instructions based on available files
    base_instructions = dedent(
        """\
        You are a helpful AI assistant built on the BeeAI framework. You have access to various tools and capabilities to assist users effectively.

        ## Core Behavior Guidelines:
        - Always be helpful, accurate, and concise in your responses
        - Use the Act Tool before executing any other tools to ensure thoughtful reasoning
        - When user requirements are unclear, use the Clarification Tool to ask specific questions
        - Maintain conversation context and refer to previous messages when relevant

        ## Tool Usage:
        - **Act Tool**: Required before using any other tool - explain your reasoning and tool choice
        - **Clarification Tool**: Ask clarifying questions when user intent is ambiguous
        - **Web Search (DuckDuckGo)**: For real-time information, current events, and general web searches
        - **Wikipedia Search**: For encyclopedic information and factual summaries
        - **Weather Tool (OpenMeteo)**: For current weather conditions and forecasts
        - **File Reader**: To read content from uploaded files (when available)
        - **File Creator**: To create new files with specific content and metadata
        - **Current Time**: For date and time information

        ## Citation Requirements:
        When using information from tools that provide URLs, you MUST cite sources using markdown format:
        - Format: [descriptive text](URL)
        - Place citations inline where information is referenced
        - Use descriptive text that summarizes the source content

        ## File Handling:
        - When files are available, reference them by ID and filename
        - Read file contents when users ask about uploaded documents
        - Create files when users need downloadable content
        {file_context}

        ## Response Quality:
        - Provide comprehensive, well-structured answers
        - Break down complex topics into digestible sections
        - Use appropriate formatting (headers, lists, code blocks) when helpful
        - Always complete tasks fully before providing final answers
        """
    )

    if extracted_files:
        files_context = "\n\n## Currently Available Files:"
        files_context += "\nThe user has uploaded the following files that you can access using the File Reader tool:"
        for file in extracted_files:
            files_context += f"\n- **{file.file.filename}** (ID: {file.file.id}) - Available at: {file.file.url}"
        files_context += (
            "\n\nWhen referencing these files, use their ID with the File Reader tool to access their content."
        )
        instructions = base_instructions.format(file_context=files_context)
    else:
        instructions = base_instructions.format(file_context="")

    # Create agent
    agent = RequirementAgent(
        llm=llm,
        tools=tools,
        memory=UnconstrainedMemory(),
        requirements=requirements,
        middlewares=[
            GlobalTrajectoryMiddleware(included=[Tool]),  # ChatModel,
            act_tool_middleware,
            clarification_tool_middleware,
        ],
    )

    messages[context.context_id].append(message)
    framework_messages[context.context_id].append(input)

    await agent.memory.add_many(framework_messages[context.context_id])
    final_answer = None

    async for event, meta in agent.run():
        if not isinstance(event, RequirementAgentSuccessEvent):
            continue

        last_step = event.state.steps[-1] if event.state.steps else None
        if last_step and last_step.tool is not None:
            trajectory_content = TrajectoryContent(
                input=last_step.input,
                output=last_step.output,
                error=last_step.error,
            )
            yield trajectory.trajectory_metadata(
                title=last_step.tool.name,
                content=trajectory_content.model_dump_json(),
            )

            if isinstance(last_step.output, FileCreatorToolOutput):
                result = last_step.output.result
                for file_info in result.files:
                    part = file_info.file.to_file_part()
                    part.file.name = file_info.display_filename
                    yield AgentArtifact(name=file_info.display_filename, parts=[part])

        if event.state.answer is not None:
            # Taking a final answer from the state directly instead of RequirementAgentRunOutput to be able to use the final answer provided by the clarification tool
            final_answer = event.state.answer

    if final_answer:
        framework_messages[context.context_id].append(final_answer)

        citations, clean_text = extract_citations(final_answer.text)

        message = AgentMessage(
            text=clean_text,
            metadata=(citation.citation_metadata(citations=citations) if citations else None),
        )
        messages[context.context_id].append(message)
        yield message


def serve():
    server.run(
        host=os.getenv("HOST", "127.0.0.1"),
        port=int(os.getenv("PORT", 8000)),
        configure_telemetry=True,
    )


if __name__ == "__main__":
    serve()

```

</details>

Source: https://github.com/i-am-bee/beeai-platform/blob/main/agents/official/beeai-framework/chat/src/chat/agent.py
