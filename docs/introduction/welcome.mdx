---
title: Welcome to Agent Stack
description: "Open infrastructure for deploying agents from any framework"
---

Agent Stack is open, self-hostable infrastructure for deploying AI agents built with any framework. Hosted by the Linux Foundation and built on the [Agent2Agent Protocol (A2A)](https://a2a-protocol.org/), it gives you everything needed to move agents from local development to shared production environments—without vendor lock-in.

## What Agent Stack Provides

Everything you need to deploy and operate agents in production:

- **A self-hostable server** that runs your agents  
- **A web UI** where users can test and share deployed agents  
- **A CLI** for deploying and managing agents  
- **Built-in infrastructure services** your agents can access at runtime:
  - **Runtime-configurable LLMs** — Switch between 15+ providers (Anthropic, OpenAI, watsonx.ai, Ollama) without changing code  
  - **Embeddings & vector search** — Power RAG and semantic search  
  - **File storage** — S3-compatible uploads and downloads  
  - **Document text extraction** — Via [Docling](https://www.docling.ai/)  
  - **External integrations** — Connect to APIs, databases, Slack, Google Drive, and more via the MCP protocol with OAuth  
  - **Secrets management** — Securely store and use API keys and credentials  
- **An SDK** (`agentstack-sdk`) that lets your agents request these services through standardized A2A extensions  
- **A HELM chart** for Kubernetes deployments with custom storage, databases, and auth  

Build your agent using any framework—LangGraph, CrewAI, BeeAI Framework, or your own—and let the SDK handle runtime service requests automatically.

## How It Works

Agent Stack provides the deployment layer between your agent code and production use:

1. **Build** your agent using `agentstack-sdk` with your preferred framework
2. **Deploy** with a single CLI command to your Agent Stack instance
3. **Users interact** through the automatically generated web interface

Your agents request infrastructure services at runtime through the A2A protocol—no hardcoded dependencies on specific LLM providers or storage solutions.

**For development:** Run Agent Stack locally with full services for rapid iteration

**For production:** Deploy to Kubernetes using the provided HELM chart and integrate with your existing infrastructure (SSO, databases, cloud storage)

## When to Use Agent Stack

Agent Stack is designed for teams who need:

- **Fast iteration** across multiple agent frameworks without committing to one vendor's hosting
- **Infrastructure control** to integrate with existing corporate systems and policies
- **Internal deployment** where you run the infrastructure rather than using external SaaS
- **Development velocity** without rebuilding deployment infrastructure for each new agent

It complements enterprise AI platforms—use it for rapid prototyping and internal agents, while enterprise systems handle governance, security, and large-scale production workflows.

## Get Started

<CardGroup cols={2}>
  <Card title="Quickstart" icon="rocket" href="/introduction/quickstart">
    Get up and running in one command
  </Card>
  <Card title="Connect A2A Agents" icon="plug" href="/introduction/connect-a2a-agents">
    Deploy existing A2A-compatible agents to Agent Stack
  </Card>
  <Card title="Start Building Agents" icon="code" href="/introduction/start-building-agents">
    Build your first agent with the Agent Stack SDK
  </Card>
  <Card title="Deploy to Production" icon="server" href="/how-to/deployment-guide">
    Deploy Agent Stack to Kubernetes for your team
  </Card>
</CardGroup>